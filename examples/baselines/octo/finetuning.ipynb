{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Octo Dataloading Examples\n",
    "\n",
    "This notebook will walk you through some of the primary features of the Octo dataloader. Data is, after all, the most important part of any machine learning pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Open X-Embodiment Data\n",
    "\n",
    "The [Open X-Embodiment (OXE)](https://robotics-transformer-x.github.io/) project was a massive cross-instutition data collection collaboration the likes of which robot learning has never seen before. The resulting dataset includes 22 different robots demonstrating 527 skills and totals over 1 million trajectories. However, as we found throughout the course of the Octo project, simply loading such a diverse set of robot data is no small feat. We hope that the `octo.data` pipeline can help kickstart anyone who hopes to take advantage of the massive collection of robot demonstrations that is OXE!\n",
    "\n",
    "### Minimum working example to load a single OXE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum working example to load a single OXE dataset\n",
    "from octo.data.oxe import make_oxe_dataset_kwargs\n",
    "from octo.data.dataset import make_single_dataset\n",
    "\n",
    "dataset_kwargs = make_oxe_dataset_kwargs(\n",
    "    # see octo/data/oxe/oxe_dataset_configs.py for available datasets\n",
    "    # (this is a very small one for faster loading)\n",
    "    \"austin_buds_dataset_converted_externally_to_rlds\",\n",
    "    # can be local or on cloud storage (anything supported by TFDS)\n",
    "    # \"/path/to/base/oxe/directory\",\n",
    "    \"gs://gresearch/robotics\",\n",
    ")\n",
    "dataset = make_single_dataset(dataset_kwargs, train=True) # load the train split\n",
    "iterator = dataset.iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_single_dataset yields entire trajectories\n",
    "traj = next(iterator)\n",
    "print(\"Top-level keys: \", traj.keys())\n",
    "print(\"Observation keys: \", traj[\"observation\"].keys())\n",
    "print(\"Task keys: \", traj[\"task\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "traj = next(iterator)\n",
    "images = traj[\"observation\"][\"image_primary\"]\n",
    "# should be: (traj_len, window_size, height, width, channels)\n",
    "# (window_size defaults to 1)\n",
    "print(images.shape)  \n",
    "Image.fromarray(np.concatenate(images.squeeze()[-5:], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should set these much higher in practice (as large as your memory can hold!)\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# turning a dataset of trajectories into a training-ready batched dataset\n",
    "train_dataset = (\n",
    "    dataset.flatten() # flattens trajectories into individual frames\n",
    "    .shuffle(SHUFFLE_BUFFER_SIZE) # shuffles the frames\n",
    "    .batch(BATCH_SIZE) # batches the frames\n",
    ")\n",
    "batch = next(train_dataset.iterator())\n",
    "images = batch[\"observation\"][\"image_primary\"]\n",
    "# should be: (batch_size, window_size, height, width, channels)\n",
    "print(images.shape)\n",
    "Image.fromarray(np.concatenate(images.squeeze()[:5], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a training-ready OXE mix\n",
    "\n",
    "In reality, you're probably going to want to mix multiple datasets together, as well as use other transformations such as resizing, augmentation, windowing, etc. This section will show you how to get a proper OXE mix up and running, as well as demonstrate additional `octo.data` features for more realistic use-cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from octo.data.oxe import make_oxe_dataset_kwargs_and_weights\n",
    "from octo.data.dataset import make_interleaved_dataset\n",
    "\n",
    "dataset_kwargs_list, sample_weights = make_oxe_dataset_kwargs_and_weights(\n",
    "    # you can pass your own list of dataset names and sample weights here, but we've\n",
    "    # also provided a few named mixes for convenience. The Octo model was trained\n",
    "    # using the \"oxe_magic_soup\" mix.\n",
    "    \"rtx\",\n",
    "    # can be local or on cloud storage (anything supported by TFDS)\n",
    "    \"gs://gresearch/robotics\",\n",
    "    # let's get a wrist camera!\n",
    "    load_camera_views=(\"primary\", \"wrist\"),\n",
    ")\n",
    "\n",
    "# see `octo.data.dataset.make_dataset_from_rlds` for the meaning of these kwargs\n",
    "dataset_kwargs_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# each element of `dataset_kwargs_list` can be used with `make_single_dataset`, but let's\n",
    "# use the more powerful `make_interleaved_dataset` to combine them for us!\n",
    "dataset = make_interleaved_dataset(\n",
    "    dataset_kwargs_list,\n",
    "    sample_weights,\n",
    "    train=True,\n",
    "    # unlike our manual shuffling above, `make_interleaved_dataset` will shuffle\n",
    "    # the JPEG-encoded images, so you should be able to fit a much larger buffer size\n",
    "    shuffle_buffer_size=SHUFFLE_BUFFER_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    # see `octo.data.dataset.apply_trajectory_transforms` for full documentation\n",
    "    # of these configuration options\n",
    "    traj_transform_kwargs=dict(\n",
    "        goal_relabeling_strategy=\"uniform\",  # let's get some goal images\n",
    "        window_size=2,  # let's get some history\n",
    "        action_horizon=4,  # let's get some future actions for action chunking\n",
    "        subsample_length=100,  # subsampling long trajectories improves shuffling a lot\n",
    "    ),\n",
    "    # see `octo.data.dataset.apply_frame_transforms` for full documentation\n",
    "    # of these configuration options\n",
    "    frame_transform_kwargs=dict(\n",
    "        # let's apply some basic image augmentations -- see `dlimp.transforms.augment_image`\n",
    "        # for full documentation of these configuration options\n",
    "        image_augment_kwargs=dict(\n",
    "            primary=dict(\n",
    "                augment_order=[\"random_resized_crop\", \"random_brightness\"],\n",
    "                random_resized_crop=dict(scale=[0.8, 1.0], ratio=[0.9, 1.1]),\n",
    "                random_brightness=[0.1],\n",
    "            )\n",
    "        ),\n",
    "        # providing a `resize_size` is highly recommended for a mixed dataset, otherwise\n",
    "        # datasets with different resolutions will cause errors\n",
    "        resize_size=dict(\n",
    "            primary=(256, 256),\n",
    "            wrist=(128, 128),\n",
    "        ),\n",
    "        # If parallelism options are not provided, they will default to tf.Data.AUTOTUNE.\n",
    "        # However, we would highly recommend setting them manually if you run into issues\n",
    "        # with memory or dataloading speed. Frame transforms are usually the speed\n",
    "        # bottleneck (due to image decoding, augmentation, and resizing), so you can set\n",
    "        # this to a very high value if you have a lot of CPU cores. Keep in mind that more\n",
    "        # parallel calls also use more memory, though.\n",
    "        num_parallel_calls=64,\n",
    "    ),\n",
    "    # Same spiel as above about performance, although trajectory transforms and data reading\n",
    "    # are usually not the speed bottleneck. One reason to manually set these is if you want\n",
    "    # to reduce memory usage (since autotune may spawn way more threads than necessary).\n",
    "    traj_transform_threads=16,\n",
    "    traj_read_threads=16,\n",
    ")\n",
    "\n",
    "# Another performance knob to tune is the number of batches to prefetch -- again,\n",
    "# the default of tf.data.AUTOTUNE can sometimes use more memory than necessary.\n",
    "iterator = dataset.iterator(prefetch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phew, that was a lot of configuration! Let's see what we got.\n",
    "batch = next(iterator)\n",
    "print(\"Top-level keys: \", batch.keys())\n",
    "# should now have \"image_primary\" and \"image_wrist\"!\n",
    "print(\"Observation keys: \", batch[\"observation\"].keys())\n",
    "# should also have \"image_primary\" and \"image_wrist\", corresponding to future goal images\n",
    "print(\"Task keys: \", batch[\"task\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "images_primary = batch[\"observation\"][\"image_primary\"]\n",
    "images_wrist = batch[\"observation\"][\"image_wrist\"]\n",
    "# should be: (batch_size, window_size (now 2), height, width, channels)\n",
    "print(images_primary.shape)\n",
    "print(images_wrist.shape)\n",
    "actions = batch[\"action\"]\n",
    "# should be: (batch_size, window_size, action_horizon, action_dim)\n",
    "print(actions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize a window of primary images\n",
    "display(Image.fromarray(np.concatenate(images_primary[0], axis=1)))\n",
    "# now a window of wrist images -- many datasets don't have wrist images,\n",
    "# so this will often be black\n",
    "display(Image.fromarray(np.concatenate(images_wrist[0], axis=1)))\n",
    "# pad_mask_dict also tells you which keys should be treated as padding\n",
    "# (e.g., if the wrist camera is black, the corresponding pad_mask_dict entry is False)\n",
    "print(batch[\"observation\"][\"pad_mask_dict\"][\"image_wrist\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the \"task\" dict: it should now have both goal\n",
    "# images and language instructions!\n",
    "goal_primary = batch[\"task\"][\"image_primary\"]\n",
    "goal_wrist = batch[\"task\"][\"image_wrist\"]\n",
    "language_instruction = batch[\"task\"][\"language_instruction\"]\n",
    "display(Image.fromarray(goal_primary[0]))\n",
    "display(Image.fromarray(goal_wrist[0]))\n",
    "print(language_instruction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script demonstrates how to finetune Octo to a new observation space (single camera + proprio)\n",
    "and new action space (bimanual) using a simulated ALOHA cube handover dataset (https://tonyzhaozh.github.io/aloha/).\n",
    "\n",
    "To run this example, first download and extract the dataset from here: https://rail.eecs.berkeley.edu/datasets/example_sim_data.zip\n",
    "\n",
    "python examples/02_finetune_new_observation_action.py --pretrained_path=hf://rail-berkeley/octo-small-1.5 --data_dir=...\n",
    "\"\"\"\n",
    "from absl import app, flags, logging\n",
    "import flax\n",
    "import jax\n",
    "import optax\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import wandb\n",
    "\n",
    "from octo.data.dataset import make_single_dataset\n",
    "from octo.model.components.action_heads import L1ActionHead\n",
    "from octo.model.components.tokenizers import LowdimObsTokenizer\n",
    "from octo.model.octo_model import OctoModel\n",
    "from octo.utils.jax_utils import initialize_compilation_cache\n",
    "from octo.utils.spec import ModuleSpec\n",
    "from octo.utils.train_utils import (\n",
    "    freeze_weights,\n",
    "    merge_params,\n",
    "    process_text,\n",
    "    TrainState,\n",
    ")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"pretrained_path\", None, \"Path to pre-trained Octo checkpoint directory.\"\n",
    ")\n",
    "flags.DEFINE_string(\"data_dir\", None, \"Path to finetuning dataset, in RLDS format.\")\n",
    "flags.DEFINE_string(\"save_dir\", None, \"Directory for saving finetuning checkpoints.\")\n",
    "flags.DEFINE_integer(\"batch_size\", 128, \"Batch size for finetuning.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"freeze_transformer\",\n",
    "    False,\n",
    "    \"Whether pre-trained transformer weights should be frozen.\",\n",
    ")\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    assert (\n",
    "        FLAGS.batch_size % jax.device_count() == 0\n",
    "    ), \"Batch size must be divisible by device count.\"\n",
    "\n",
    "    initialize_compilation_cache()\n",
    "    # prevent tensorflow from using GPU memory since it's only used for data loading\n",
    "    tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "    # setup wandb for logging\n",
    "    wandb.init(name=\"finetune_aloha\", project=\"octo\")\n",
    "\n",
    "    # load pre-trained model\n",
    "    logging.info(\"Loading pre-trained model...\")\n",
    "    pretrained_model = OctoModel.load_pretrained(FLAGS.pretrained_path)\n",
    "\n",
    "    # make finetuning dataset\n",
    "    # apply Gaussian normalization, load chunks of 50 actions since we'll train with action chunking\n",
    "    # delete goal images in the data loader since we will train a language-conditioned-only policy\n",
    "    # TODO: directly load this from raw data to make it less opaque?\n",
    "    logging.info(\"Loading finetuning dataset...\")\n",
    "    dataset = make_single_dataset(\n",
    "        dataset_kwargs=dict(\n",
    "            name=\"aloha_sim_cube_scripted_dataset\",\n",
    "            data_dir=FLAGS.data_dir,\n",
    "            image_obs_keys={\"primary\": \"top\"},\n",
    "            proprio_obs_key=\"state\",\n",
    "            language_key=\"language_instruction\",\n",
    "        ),\n",
    "        traj_transform_kwargs=dict(\n",
    "            window_size=1,\n",
    "            action_horizon=50,\n",
    "        ),\n",
    "        frame_transform_kwargs=dict(\n",
    "            resize_size={\"primary\": (256, 256)},\n",
    "        ),\n",
    "        train=True,\n",
    "    )\n",
    "    train_data_iter = (\n",
    "        dataset.repeat()\n",
    "        .unbatch()\n",
    "        .shuffle(10000)  # can reduce this if RAM consumption too high\n",
    "        .batch(FLAGS.batch_size)\n",
    "        .iterator()\n",
    "    )\n",
    "\n",
    "    # run text tokenizer over batch (this needs to happen before training / sharding) + delete unused keys\n",
    "    text_processor = pretrained_model.text_processor\n",
    "\n",
    "    def process_batch(batch):\n",
    "        batch = process_text(batch, text_processor)\n",
    "        del batch[\"dataset_name\"]\n",
    "        return batch\n",
    "\n",
    "    train_data_iter = map(process_batch, train_data_iter)\n",
    "    example_batch = next(train_data_iter)\n",
    "\n",
    "    # load pre-training config and modify --> remove wrist cam, add proprio input, change action head\n",
    "    # following Zhao et al. we use \"action chunks\" of length 50 and L1 loss for ALOHA\n",
    "    config = pretrained_model.config\n",
    "    del config[\"model\"][\"observation_tokenizers\"][\"wrist\"]\n",
    "    ###\n",
    "    config[\"model\"][\"observation_tokenizers\"][\"proprio\"] = ModuleSpec.create(\n",
    "        LowdimObsTokenizer,\n",
    "        n_bins=256,\n",
    "        bin_type=\"normal\",\n",
    "        low=-2.0,\n",
    "        high=2.0,\n",
    "        obs_keys=[\"proprio\"],\n",
    "    )\n",
    "    # Fully override the old action head with a new one (for smaller changes, you can use update_config)\n",
    "    config[\"model\"][\"heads\"][\"action\"] = ModuleSpec.create(\n",
    "        L1ActionHead,\n",
    "        action_horizon=50,\n",
    "        action_dim=14,\n",
    "        readout_key=\"readout_action\",\n",
    "    )\n",
    "\n",
    "    # initialize weights for modified Octo model, then merge in all applicable pre-trained weights\n",
    "    # new position encodings for proprio inputs & weights for new action head will remain \"from scratch\"\n",
    "    logging.info(\"Updating model for new observation & action space...\")\n",
    "    model = OctoModel.from_config(\n",
    "        config,\n",
    "        example_batch,\n",
    "        text_processor,\n",
    "        verbose=True,\n",
    "        dataset_statistics=dataset.dataset_statistics,\n",
    "    )\n",
    "    merged_params = merge_params(model.params, pretrained_model.params)\n",
    "    # can perform any additional parameter surgery here...\n",
    "    # ...\n",
    "    model = model.replace(params=merged_params)\n",
    "    del pretrained_model\n",
    "\n",
    "    # create optimizer & train_state, optionally freeze keys for pre-trained transformer\n",
    "    # train_state bundles parameters & optimizers\n",
    "    learning_rate = optax.join_schedules(\n",
    "        [optax.linear_schedule(0, 3e-5, 100), optax.constant_schedule(3e-5)], [100]\n",
    "    )\n",
    "    tx = optax.adamw(learning_rate)\n",
    "    frozen_keys = model.config[\"optimizer\"][\"frozen_keys\"]\n",
    "    if FLAGS.freeze_transformer:\n",
    "        frozen_keys.append(\"BlockTransformer_0\")\n",
    "    tx = freeze_weights(tx, model.params, frozen_keys)\n",
    "    train_state = TrainState.create(\n",
    "        rng=jax.random.PRNGKey(1234),\n",
    "        model=model,\n",
    "        tx=tx,\n",
    "    )\n",
    "\n",
    "    # define loss function and train step\n",
    "    def loss_fn(params, batch, rng, train=True):\n",
    "        bound_module = model.module.bind({\"params\": params}, rngs={\"dropout\": rng})\n",
    "        transformer_embeddings = bound_module.octo_transformer(\n",
    "            batch[\"observation\"],\n",
    "            batch[\"task\"],\n",
    "            batch[\"observation\"][\"timestep_pad_mask\"],\n",
    "            train=train,\n",
    "        )\n",
    "        action_loss, action_metrics = bound_module.heads[\"action\"].loss(\n",
    "            transformer_embeddings,  # Action head knows to pull out the action readout_key\n",
    "            batch[\"action\"],\n",
    "            batch[\"observation\"][\"timestep_pad_mask\"],\n",
    "            batch[\"action_pad_mask\"],\n",
    "            train=train,\n",
    "        )\n",
    "        return action_loss, action_metrics\n",
    "\n",
    "    @jax.jit\n",
    "    def train_step(state, batch):\n",
    "        rng, dropout_rng = jax.random.split(state.rng)\n",
    "        (loss, info), grads = jax.value_and_grad(loss_fn, has_aux=True)(\n",
    "            state.model.params, batch, dropout_rng, train=True\n",
    "        )\n",
    "        new_state = state.apply_gradients(grads=grads, rng=rng)\n",
    "        return new_state, info\n",
    "\n",
    "    # run finetuning loop\n",
    "    print(\"Starting finetuning...\")\n",
    "    for i in tqdm.tqdm(range(5000), total=5000, dynamic_ncols=True):\n",
    "        batch = next(train_data_iter)\n",
    "        train_state, update_info = train_step(train_state, batch)\n",
    "        if (i + 1) % 100 == 0:\n",
    "            update_info = jax.device_get(update_info)\n",
    "            wandb.log(\n",
    "                flax.traverse_util.flatten_dict({\"training\": update_info}, sep=\"/\"),\n",
    "                step=i,\n",
    "            )\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            # save checkpoint\n",
    "            train_state.model.save_pretrained(step=i, checkpoint_path=FLAGS.save_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Octo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "BOOTSTRAP_TRAJECTORIES_PATH: str = \"/home/oop/dev/ManiSkill/examples/baselines/ppo/runs/pd_ee_delta_pose/test_videos/trajectory\"\n",
    "BOOTSTRAP_TRAJECTORIES_PATH_JSON: str = f\"{BOOTSTRAP_TRAJECTORIES_PATH}.json\"\n",
    "BOOTSTRAP_TRAJECTORIES_PATH_H5: str = f\"{BOOTSTRAP_TRAJECTORIES_PATH}.h5\"\n",
    "\n",
    "# Open the file\n",
    "with h5py.File(BOOTSTRAP_TRAJECTORIES_PATH_H5, 'r') as file:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % file.keys())\n",
    "    # Optionally, iterate through the file and display each item\n",
    "    for key in file.keys():\n",
    "        print(f\"Key: {key}\")\n",
    "        data = file[key]\n",
    "        print(\"Data keys: %s\" % data.keys())\n",
    "        for data_key in data.keys():\n",
    "            print(f\"Data key: {data_key}\")\n",
    "            data_data = data[data_key]\n",
    "            print(f\"Data data: {data_data}\")\n",
    "            print(f\"Data data shape: {data_data.shape}\")\n",
    "            print(f\"Data data type: {data_data.dtype}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Convert the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "octo-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
